{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1afd22d0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Calculating Network Error with Loss\n",
    "To Begin adjusting ***weights*** and ***biases*** to decrease error over time, our next step is to quantify how wrong the model is through whats defined as a ***loss function***.\n",
    "***accuracy*** is simply applying an argmax to the output to find the index of the biggest value\n",
    "We’re not performing regression in this example; we’re classifying, so we need a different loss function. The model has a softmax activation function for the output layer, which means it’s outputting a probability distribution\n",
    "\n",
    "##### Categorical Cross-Entropy Loss\n",
    "Categorical cross-entropy is explicitly used to compare a ground-truth probability (y or targets) and some predicted distribution (y-hat o r predictions), so it makes sense to use cross-entropy here. It is also one of the most commonly used loss functions with a softmax activation on the output layer.\n",
    "<div>\n",
    "<img src=\"images/image5-6.1.png\" width=\"400\"/>\n",
    "</div>\n",
    "Where Li denotes sample loss value, i is the i-th sample in the set, j is the label/output index, y denotes the target values, and y-hat denotes the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f62b6e2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# An example output from the output layer of the neural network\n",
    "softmax_output = [0.7, 0.1, 0.2] # Ground truth\n",
    "target_output = [1, 0, 0]\n",
    "loss = -(math.log(softmax_output[0])*target_output[0] +\n",
    "         math.log(softmax_output[1])*target_output[1] +\n",
    "         math.log(softmax_output[2])*target_output[2])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7695d249",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "the equation can be simplified into:\n",
    "loss = -math.log(softmax_output[0]) because target_output[1] and target_output[2] are 0. This is because targets can be one-hot encoded, where all values, except for one, are zeros, and the correct label’s position is filled with 1\n",
    "\n",
    "Log is short for logarithm and is defined as the solution for the x-term in an equation of the form a^x = b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62fb0449",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333316 0.3333332  0.33333364]\n",
      " [0.33333287 0.3333329  0.33333418]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "loss: 1.0986104\n"
     ]
    }
   ],
   "source": [
    "# pip install numpy nnfs\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    # Initialize weights and biases\n",
    "    def __init__(self, n_inputs, n_neurons) :\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    # Forward pass\n",
    "    # Calculate output values from inputs, weights and biases\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs self.output = np.maximum(0, inputs)\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "# Softmax activation\n",
    "class Activation_Softmax: # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "        self.output = probabilities   \n",
    "        \n",
    "# Common loss class\n",
    "class Loss:\n",
    "    # Calculates the data and regularization losses # given model output and ground truth values\n",
    "    def calculate(self, output, y) :\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss \n",
    "        return data_loss\n",
    "    \n",
    "# Cross-entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true) :\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Probabilities for target values - # only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "            \n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis= 1)\n",
    "            \n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    \n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 3 input features (as we take output # of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Make a forward pass through activation function\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Make a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "\n",
    "\n",
    "# Perform a forward pass through loss function\n",
    "# it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "# Print loss value\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fd4cdf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Accuracy Calculation\n",
    "While loss is a useful metric for optimizing a model, the metric commonly used in practice along with loss is the ***accuracy***, which describes how often the largest confidence is the correct class in terms of a fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1888452d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Probabilities of 3 samples\n",
    "softmax_outputs = np.array([[0.7, 0.2, 0.1], [0.5, 0.1, 0.4],[0.02, 0.9, 0.08]])\n",
    "\n",
    "# Target (ground-truth) labels for 3 samples\n",
    "class_targets = np.array([0, 1, 1])\n",
    "\n",
    "# Calculate values along second axis (axis of index 1)\n",
    "predictions = np.argmax(softmax_outputs, axis=1) \n",
    "\n",
    "# If targets are one-hot encoded - convert them \n",
    "if len(class_targets.shape) == 2:\n",
    "    class_targets = np.argmax(class_targets, axis= 1) # True evaluates to 1; False to 0\n",
    "    \n",
    "accuracy = np.mean(predictions==class_targets)\n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1de34fd6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.34\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy from output of activation2 and targets\n",
    "# Calculate values along first axis\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions==y)\n",
    "# Print accuracy\n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8a5c5e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that you’ve learned how to perform a forward pass through our network and calculate the metrics to signal if the model is performing poorly, we will embark on optimization\n",
    "## Optimization\n",
    "Now that the neural network is built, able to have data passed through it, and capable of calculating loss, the next step is to determine how to adjust the weights and biases to decrease the loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}