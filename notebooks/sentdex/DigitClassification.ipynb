{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Class Session::\n",
    "https://www.youtube.com/watch?v=OMDn66kM9Qc&ab_channel=LightningAI\n",
    "# MNIST Digital Classification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# install m1 gpu support\n",
    "# pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "macOS-12.3.1-arm64-arm-64bit\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import torch\n",
    "from torch import nn\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# test gpu run\n",
    "torch.backends.mps.is_available()\n",
    "torch.randn(5, device=device)\n",
    "print(torch.has_mps)\n",
    "print(platform.platform())\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "ResNet2(\n",
      "  (l1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (l2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (l3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (do): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Fully Connected Neurol Network\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Fully Connected Neurol Network with Residual Connections\n",
    "# Faster Training with this network\n",
    "class ResNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(28*28, 64)\n",
    "        self.l2 = nn.Linear(64, 64)\n",
    "        self.l3 = nn.Linear(64, 10)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        do = self.do(h2 + h1)\n",
    "        logits = self.l3(do)\n",
    "        return logits\n",
    "\n",
    "print(ResNet())\n",
    "print(ResNet2())\n",
    "model = ResNet2()\n",
    "#model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ResNet\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,10)\n",
    ")\n",
    "In this Fully Connected Neurol Network we have an input layer that takes an image of 28*28px and transforms it to an output of 64. This is the hidden dimension and only the network knows what's going on inside. Next we have a non-linear function such that we are learning something.\n",
    "\n",
    "Next we have a 2nd hidden layer, with 64 dimension input and output. Then we learn again with a Relu function. Finally, we have our output layer that takes a hidden dimension and outputs our 10 classification classes.\n",
    "\n",
    "## ResNet2\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(64,10)\n",
    ")\n",
    "In this Fully Connected Neurol Network we use Residual Connections to speed up training time. Again we start off with an input layer, then we have a number of hidden layers. The key difference is the Dropout function, which drastically reduces the chance of overfitting during training.\n",
    "More info:: https://wandb.ai/authors/ayusht/reports/Implementing-Dropout-in-PyTorch-With-Example--VmlldzoxNTgwOTE\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# optimiser\n",
    "params = model.parameters()\n",
    "optimiser = optim.SGD(params,lr=1e-2)\n",
    "\n",
    "# loss function\n",
    "loss = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we create our optimiser, and define our loss function. Optimization is the process of adjusting model parameters to reduce model error in each training step. Our Optimiser here is a stochastic gradient decent with a learning rate of 0.01. Loss functions are used to gauge the error between the prediction output and the provided target value. A loss function tells us how far the algorithm model is from realizing the expected outcome. Our Loss function here is a cross entropy loss.\n",
    "## Notes highlight\n",
    "CrossEntropyLoss - > good loss functions for classification problems\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "train_data = datasets.MNIST('', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ]))\n",
    "train, val = random_split(train_data,[55000,5000])\n",
    "train_loader = DataLoader(train, batch_size=32)\n",
    "val_loader = DataLoader(val,batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download our training data from MNIST datasets with train True (meaning our data should be training data)\n",
    "Split our training data into a training part, and validation part\n",
    "\n",
    "Make our data iteraterable with DataLoader\n",
    "\n",
    "\n",
    "Download our testing data from MNIST datasets with train False (meaning our data should not be training data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss: 0.87\n",
      "Epoch 1, validation loss: 0.46\n",
      "Epoch 2, train loss: 0.38\n",
      "Epoch 2, validation loss: 0.37\n",
      "Epoch 3, train loss: 0.32\n",
      "Epoch 3, validation loss: 0.32\n",
      "Epoch 4, train loss: 0.28\n",
      "Epoch 4, validation loss: 0.28\n",
      "Epoch 5, train loss: 0.25\n",
      "Epoch 5, validation loss: 0.26\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "# number of iteration through dataset\n",
    "nb_epochs = 5\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    losses = list()\n",
    "    for batch in train_loader:\n",
    "        x,y = batch # separate image matrix from our class label\n",
    "        #x, y = x.to(device), y.to(device)\n",
    "\n",
    "        #x: image matrix batch_size x 1 x 28 x 28\n",
    "        b = x.size(0)\n",
    "        x = x.view(b,-1) # convert to matrix with b rows, and column (1 x 28 * 28) = 784\n",
    "\n",
    "        # step 1: forward\n",
    "        l = model(x) # logits:: output of our last layer\n",
    "\n",
    "        # step 2: compute objective function\n",
    "        J = loss(l,y)\n",
    "\n",
    "        # step 3: clean up gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # step 4: accumulate the partial derivatives of J with respect to parameters\n",
    "        J.backward()\n",
    "\n",
    "        # step 5: learn - apply our optimiser step [ opposite direction of gradient]\n",
    "        optimiser.step()\n",
    "\n",
    "        # compute losses\n",
    "        losses.append(J.item())\n",
    "\n",
    "    # plot with 2 decimal places\n",
    "    print(f'Epoch {epoch + 1}, train loss: {torch.tensor(losses).mean():.2f}')\n",
    "\n",
    "    losses = list()\n",
    "    for batch in val_loader:\n",
    "        x,y = batch # separate image matrix from our class label\n",
    "        #x, y = x.to(device), y.to(device)\n",
    "\n",
    "        #x: image matrix batch_size x 1 x 28 x 28\n",
    "        b = x.size(0)\n",
    "        x = x.view(b,-1) # convert to matrix with b rows, and column (1 x 28 * 28) = 784\n",
    "\n",
    "        # step 1: forward\n",
    "        with torch.no_grad():\n",
    "            l = model(x) # logits:: output of our last layer\n",
    "\n",
    "        J = loss(l, y)\n",
    "\n",
    "        losses.append(J.item())\n",
    "\n",
    "    # plot with 2 decimal places\n",
    "    print(f'Epoch {epoch + 1}, validation loss: {torch.tensor(losses).mean():.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We iterate through our training data 5x\n",
    "In each step::\n",
    "    In our training loop we iterate through our validation data\n",
    "        extract one batch from our training data\n",
    "        separate our image x, from our class label y from the batch\n",
    "        convert our batch image matrix to a matrix that fits our input\n",
    "        forward our image into our input layer in the model\n",
    "        compute the objective function (last layer output) - When Loss is calcul Output will be a tensor without a graph [saves memory]\n",
    "        clean up our gradients before computing our derivatives\n",
    "        compute partial derivatives (clean up gradients otherwise derivatives will accumulate)\n",
    "        step into our optimiser, the learning part\n",
    "        append our loss to our training loss list\n",
    "        Finally, we print our losses\n",
    "    In our validation loop we iterate through our validation data\n",
    "        extract one batch from our training data\n",
    "        separate our image x, from our class label y from the batch\n",
    "        convert our batch image matrix to a matrix that fits our input\n",
    "        forward our image into our input layer in the model\n",
    "            our output will not have a graph attached to each tensor\n",
    "        compute the objective function (last layer output) - When Loss is calcul Output will be a tensor without a graph [saves memory]\n",
    "        append to our validation loss list\n",
    "    Print our validation loss\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "jupyter",
   "language": "python",
   "display_name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}