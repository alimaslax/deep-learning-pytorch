{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2730178a",
   "metadata": {},
   "source": [
    "## Activication Function\n",
    "The activation function is applied to the output of a neuron (or layer of neurons), which modifies outputs.We use activation functions because if the activation function itself is nonlinear, it allows for neural networks with usually two or more hidden layers to map nonlinear functions\n",
    "\n",
    "In general, your neural network will have ***two types*** of activation functions. The first will be the activation function used in hidden layers, and the second will be used in the output layer.\n",
    "\n",
    "### Type 1: hidden layer activation\n",
    "###### 1 Linear Activation Function\n",
    "A linear function is simply the equation of a line. It will appear as a straight line when graphed, where y=x and the output value equals the input.\n",
    "<div>\n",
    "<img src=\"images/image4.1.png\" width=\"400\"/>\n",
    "</div>\n",
    "This activation function is usually applied to the last layer’s output in the case of a regression model — a model that outputs a scalar value instead of a classification\n",
    "\n",
    "###### 2 (OutDated). The Step Activation Function\n",
    "this activation function serves is to mimic a neuron “firing” or “not firing” based on input information. The simplest version of this is a step function. In a single neuron, if the ​weights · inputs + bias results in a value greater than 0, the neuron will fire and output a 1; otherwise, it will output a 0.\n",
    "<div>\n",
    "<img src=\"images/image4.2.png\" width=\"400\"/>\n",
    "</div>\n",
    "This activation function has been used historically in hidden layers, ***but nowadays, it is rarely a choice.***\n",
    "\n",
    "###### 3 The Sigmoid Activation Function\n",
    "The problem with a step function is that its less clear to the optimizer what these impacts are because theres very little information gathered from this function. Neurons are either dead or alive(Its either on (1) or off (0)). The original, more granular, activation function used for neural networks was the Sigmoid activation function, which looks like:\n",
    "<div>\n",
    "<img src=\"images/image4.3.png\" width=\"400\"/>\n",
    "</div>\n",
    "This function returns a value in the range of 0 for negative infinity, through 0.5 for the input of 0, and to 1 for positive infinity. In this case, were getting a value that can be reversed to its original value; the returned value contains all the information from the input, contrary to a function like the step function, where an input of 3 will output the same value as an input of 300,000. The Sigmoid function, historically used in hidden layers, was eventually replaced by the Rectified Linear Units​ activation function (or ​ReLU​)\n",
    "\n",
    "###### The Rectified Linear Activation Function (ReLU)\n",
    "y=x, clipped at 0 from the negative side. If x is less than or equal to 0, then y is 0  otherwise, y is equal to x. The ReLU activation function is extremely close to being a linear activation function while remaining nonlinear, due to that bend after 0. This simple property is, however, very effective.\n",
    "<div>\n",
    "<img src=\"images/image4.4.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "## Why Use Activaction Functions\n",
    "In most cases, for a neural network to fit a nonlinear function, we need it to contain two or more hidden layers, and we need those hidden layers to use a nonlinear activation function. Why?\n",
    "\n",
    "No matter what we do with this neuron’s weights and biases in a linear activation function, the output of this neuron will be perfectly linear to y=x. This linear nature will continue throughout the entire network\n",
    "<div>\n",
    "<img src=\"images/image4.5.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "When using the same 2 hidden layers of 8 neurons each with the rectified linear activation function, or any other non linear activation function, we see the following result after training (note: ReLU is barely nonlinear):\n",
    "<div>\n",
    "<img src=\"images/image4.6.png\" width=\"400\"/>\n",
    "</div>\n",
    "In the image above the weights and bias for each input can be adjusted so that our final output model fits our non-linear relationship. If we kept 2 hidden layers but changed the 8 neurons to 64 neurons we see further improvement\n",
    "<div>\n",
    "<img src=\"images/image4.7.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73a4b986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "# ReLU Activation Function Code\n",
    "# x > 0 return x, x < 0 return 0\n",
    "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
    "output = []\n",
    "for i in inputs:\n",
    "    output.append(max(0, i))\n",
    "    \n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d9b887",
   "metadata": {},
   "source": [
    "Let’s talk about the activation function that we are going to use on the output of the last layer\n",
    "\n",
    "### Type 2: output layer activation\n",
    "First, why are we bothering with another activation function? It just depends on what our overall goals are. In this case, the rectified linear unit is unbounded, not normalized with other units, and exclusive. “Not normalized” implies the values can be anything, an output of [12, 99, 318] is without context, and “exclusive” means each output is independent of the others. \n",
    "###### The Softmax Activation Function\n",
    "<div>\n",
    "<img src=\"images/image4.8.png\" width=\"400\"/>\n",
    "</div>\n",
    "Softmax activation function is meant for classification problems. To address this lack of context, the softmax activation on the output data can take in non-normalized, or uncalibrated, inputs and produce a normalized distribution of probabilities for our classes. In the case of classification, what we want to see is a prediction of which class the network “thinks” the input represents. This distribution returned by the softmax activation function represents ​confidence scores​ for each class and will add up to 1. For example, if our network has a confidence distribution for two classes: [0.45, 0.55], the prediction is the 2nd class, but the confidence in this prediction isn’t very high. Maybe our program would not act in this case since it’s not very confident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01b291ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponentiated values:\n",
      "[121.51041752   3.35348465  10.85906266]\n",
      "normalized exponentiated values:\n",
      "[0.89528266 0.02470831 0.08000903]\n",
      "sum of normalized values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# For each value in a vector, calculate the exponential value\n",
    "exp_values = np.exp(layer_outputs) \n",
    "print('exponentiated values:')\n",
    "print(exp_values)\n",
    "\n",
    "# Now normalize values\n",
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print('normalized exponentiated values:')\n",
    "print(norm_values)\n",
    "print('sum of normalized values:', np.sum(norm_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed9ef2d",
   "metadata": {},
   "source": [
    "Equation for Softmax::\n",
    "Step 1. “exponentiate” the outputs\n",
    "***The exponential function is a monotonic function***. This means that, with higher input values, outputs are also higher, so we won’t change the predicted class after applying it while making sure that we get non-negative values.\n",
    "\n",
    "Step 2. convert these numbers to a probability distribution (Normalization)\n",
    "take a given exponentiated value and divide it by the sum of all of the exponentiated values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc8916b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333316 0.3333332  0.33333364]\n",
      " [0.33333287 0.3333329  0.33333418]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n"
     ]
    }
   ],
   "source": [
    "# pip install numpy nnfs\n",
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()\n",
    "\n",
    "class Layer_Dense:\n",
    "    # Initialize weights and biases\n",
    "    def __init__(self, n_inputs, n_neurons) :\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    # Forward pass\n",
    "    # Calculate output values from inputs, weights and biases\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        \n",
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Calculate output values from inputs self.output = np.maximum(0, inputs)\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "# Softmax activation\n",
    "class Activation_Softmax: # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "        self.output = probabilities       \n",
    "        \n",
    "        \n",
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 3 input features (as we take output # of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "# Make a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "# Make a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Make a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365f4e87",
   "metadata": {},
   "source": [
    "We used the ***Rectified Linear (ReLU) activation function*** on the hidden layer, which works on a per-neuron basis. We additionally used the ***Softmax activation function*** for the output layer since it accepts non-normalized values as input and outputs a probability distribution, which were using as confidence scores for each class.\n",
    "To Begin adjusting ***weights*** and ***biases*** to decrease error over time, our next step is to quantify how wrong the model is through whats defined as a ***loss function***.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
