{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2730178a",
   "metadata": {},
   "source": [
    "#### Activication Function\n",
    "The activation function is applied to the output of a neuron (or layer of neurons), which modifies outputs.We use activation functions because if the activation function itself is nonlinear, it allows for neural networks with usually two or more hidden layers to map nonlinear functions\n",
    "\n",
    "In general, your neural network will have ***two types*** of activation functions. The first will be the activation function used in hidden layers, and the second will be used in the output layer.\n",
    "\n",
    "###### 1 Linear Activation Function\n",
    "A linear function is simply the equation of a line. It will appear as a straight line when graphed, where y=x and the output value equals the input.\n",
    "<div>\n",
    "<img src=\"images/image4-5.1.png\" width=\"400\"/>\n",
    "</div>\n",
    "This activation function is usually applied to the last layer’s output in the case of a regression model — a model that outputs a scalar value instead of a classification\n",
    "\n",
    "###### 2 (OutDated). The Step Activation Function\n",
    "this activation function serves is to mimic a neuron “firing” or “not firing” based on input information. The simplest version of this is a step function. In a single neuron, if the ​weights · inputs + bias results in a value greater than 0, the neuron will fire and output a 1; otherwise, it will output a 0.\n",
    "<div>\n",
    "<img src=\"images/image4-5.2.png\" width=\"400\"/>\n",
    "</div>\n",
    "This activation function has been used historically in hidden layers, ***but nowadays, it is rarely a choice.***\n",
    "\n",
    "###### 3 The Sigmoid Activation Function\n",
    "The problem with a step function is that its less clear to the optimizer what these impacts are because theres very little information gathered from this function. Neurons are either dead or alive(Its either on (1) or off (0)). The original, more granular, activation function used for neural networks was the Sigmoid activation function, which looks like:\n",
    "<div>\n",
    "<img src=\"images/image4-5.3.png\" width=\"400\"/>\n",
    "</div>\n",
    "This function returns a value in the range of 0 for negative infinity, through 0.5 for the input of 0, and to 1 for positive infinity. In this case, were getting a value that can be reversed to its original value; the returned value contains all the information from the input, contrary to a function like the step function, where an input of 3 will output the same value as an input of 300,000. The Sigmoid function, historically used in hidden layers, was eventually replaced by the Rectified Linear Units​ activation function (or ​ReLU​)\n",
    "\n",
    "###### The Rectified Linear Activation Function\n",
    "y=x, clipped at 0 from the negative side. If x is less than or equal to 0, then y is 0  otherwise, y is equal to x. The ReLU activation function is extremely close to being a linear activation function while remaining nonlinear, due to that bend after 0. This simple property is, however, very effective.\n",
    "<div>\n",
    "<img src=\"images/image4-5.4.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "## Why Use Activaction Functions\n",
    "In most cases, for a neural network to fit a nonlinear function, we need it to contain two or more hidden layers, and we need those hidden layers to use a nonlinear activation function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
